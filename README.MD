#  core-etl-api-to-s3-airflow

Este proyecto automatiza un pipeline de datos que extrae informaci贸n desde una API p煤blica, la guarda en una base de datos SQLite, luego la transforma a CSV usando `pandas`, y finalmente la sube a AWS S3.

> О Usa una biblioteca propia en Python: [`core-extract-and-load`](https://pypi.org/project/core-extract-and-load/) para las operaciones de extracci贸n y guardado.

> 锔 Todo el proceso est谩 orquestado mediante un DAG de Apache Airflow y ejecutado en contenedores Docker.

---

##  Tabla de contenido

- [ Sobre el proyecto](#-sobre-el-proyecto)
- [ Tecnolog铆as](#-tecnolog铆as)
- [ Instalaci贸n](#-instalaci贸n)
- [ Variables de entorno](#-variables-de-entorno)
- [ Uso](#-uso)
- [ Estructura del proyecto](#-estructura-del-proyecto)
- [ Pipeline de datos](#-pipeline-de-datos)
- [И Ejemplo de ejecuci贸n](#И-ejemplo-de-ejecuci贸n)
- [ Licencia](#-licencia)
- [锔 Autor](#锔-autor)
- [ Recursos](#-recursos)

---

##  Sobre el proyecto

Este proyecto fue desarrollado como parte de una prueba t茅cnica para demostrar habilidades en:

- Automatizaci贸n de pipelines ETL
- Desarrollo de bibliotecas propias en Python
- Transformaci贸n de datos con `pandas`
- Almacenamiento en AWS S3
- Orquestaci贸n con Apache Airflow

---

##  Tecnolog铆as

- Python 3.12
- Apache Airflow
- Docker / Docker Compose
- SQLite
- pandas
- Amazon S3
- [`core-extract-and-load`](https://pypi.org/project/core-extract-and-load/)

---

##  Instalaci贸n

```bash
# Clonar el repositorio
git clone https://github.com/facuvgaa/core-etl-api-to-s3-airflow.git
cd core-etl-api-to-s3-airflow

# Crear archivo de variables de entorno
cp .env.example .env
# Editar el archivo .env con tus credenciales de AWS

# Levantar Airflow con Docker
docker-compose up -d